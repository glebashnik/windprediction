{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pandas import read_csv, DataFrame, concat\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Fix on windows bug from the Nov 2017 update\n",
    "import win_unicode_console\n",
    "win_unicode_console.enable()\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, datapath):\n",
    "        self.datapath = datapath\n",
    "        try:\n",
    "            self.dataset = read_csv(self.datapath, index_col=0, sep=';')\n",
    "            self.dataset.index.name = 'index'\n",
    "            print('Data loaded, shape: ' + str(self.dataset.shape))\n",
    "        except:\n",
    "            print('No data found on: ' + self.datapath)\n",
    "            exit(1)\n",
    "\n",
    "        self.dataset = self.dataset.dropna()\n",
    "\n",
    "        self.testsplit = 0.8\n",
    "        self.batch_size = 32\n",
    "        self.epochs = 100\n",
    "\n",
    "        data_x = self.dataset.iloc[:, :-1]\n",
    "        data_y = self.dataset.iloc[:, -1:]\n",
    "\n",
    "        #Normalizing data\n",
    "        self.x_scaler = MinMaxScaler(copy=True, feature_range=(0,1))\n",
    "        data_x = self.x_scaler.fit_transform(data_x)\n",
    "\n",
    "        self.dataset.iloc[:, :-1] = data_x\n",
    "        self.dataset.iloc[:, -1:] = data_y\n",
    "        self.data = np.concatenate((data_x, data_y), axis=1)\n",
    "\n",
    "        # Number of timesteps we want to look back and on\n",
    "        n_in = 4\n",
    "        n_out = 0\n",
    "\n",
    "        # Returns an (n_in * n_out) * num_vars NDFrame\n",
    "        self.timeseries = self.series_to_supervised(data=self.data, \n",
    "                n_in=n_in, \n",
    "                n_out=n_out,\n",
    "                dropnan=True)\n",
    "\n",
    "        # Converts to numpy representation\n",
    "        self.timeseries_np = self.timeseries.values\n",
    "\n",
    "        # Reshape to three dimensions (number of samples x number of timesteps x number of variables)\n",
    "        self.timeseries_data = self.timeseries_np.reshape(self.timeseries_np.shape[0], n_in+n_out ,self.data.shape[1])\n",
    "\n",
    "        # Data is everything but the two last rows in the third dimension (which contain the delayed and actual production values)\n",
    "        self.x_data = self.timeseries_data[:, :, :-1]\n",
    "        self.y_data = self.timeseries_data[:, :, -1:]\n",
    "\n",
    "        #Split for dividing the dataset in a factor of the batch size\n",
    "        split = int(self.testsplit * self.x_data.shape[0])\n",
    "\n",
    "        # Create training and test sets for x\n",
    "        self.x_train = self.x_data[:split, :, :]\n",
    "        self.x_test = self.x_data[split:, :, :]\n",
    "\n",
    "        # Create training and test sets for y\n",
    "        self.y_train = self.y_data[:split, 0, :]\n",
    "        self.y_test = self.y_data[split:, 0, :]\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(LSTM(32, return_sequences=True, input_shape=(self.x_train.shape[1], self.x_train.shape[2])))\n",
    "\n",
    "        self.model.add(LSTM(16))\n",
    "\n",
    "        self.model.add(Dense(1))\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "    def train_network(self):\n",
    "        self.model.compile(loss='mae', optimizer='adam', metrics=['mae','mse',self.rmse])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "        checkpoint = ModelCheckpoint('checkpoint_model.h5', monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "        # Creates the closest validation splitt divisible by batch size to 0.2\n",
    "        samples_split = self.x_train.shape[0]\n",
    "        val_split = ((0.2 * samples_split - ((0.2 * samples_split) % self.batch_size))/samples_split)\n",
    "\n",
    "\n",
    "        self.model.fit(x=self.x_train, y=self.y_train, batch_size=self.batch_size, callbacks=[early_stopping, checkpoint], validation_split=val_split, epochs = self.epochs, verbose=1, shuffle=False)\n",
    "\n",
    "    def predict(self):\n",
    "        # Load best found model\n",
    "        self.model.load_weights(os.path.join('checkpoint_model.h5'))\n",
    "\n",
    "        self.predictions = self.model.predict(self.x_test, batch_size=self.batch_size)\n",
    "\n",
    "        self.evaluation = self.model.evaluate(self.x_test, self.y_test, batch_size=self.batch_size)\n",
    "\n",
    "        print('Evaluating with test data')\n",
    "        print(self.model.metrics_names)\n",
    "        print(self.evaluation)\n",
    "        print()\n",
    "\n",
    "    #RMSE loss function (missing in keras library)\n",
    "    def rmse_numpy(self, y_true, y_pred):\n",
    "        return np.sqrt(np.mean(np.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "    def rmse(self, y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "    # convert series to supervised learning, time series data generation\n",
    "    def series_to_supervised(self, data, n_in=1, n_out=1, dropnan=True):\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        df = DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "        # input sequence (t-n, ... t-1)\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "        # forecast sequence (t, t+1, ... t+n)\n",
    "        for i in range(0, n_out):\n",
    "            cols.append(df.shift(-i))\n",
    "            if i == 0:\n",
    "                names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "            else:\n",
    "                names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "        # put it all together\n",
    "        agg = concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # drop rows with NaN values\n",
    "        if dropnan:\n",
    "            agg.dropna(inplace=True)\n",
    "        return agg\n",
    "\n",
    "    def visualize(self):\n",
    "        lines = pyplot.plot(self.predictions, 'r', self.y_test, 'b')\n",
    "        pyplot.setp(lines, linewidth=0.5)\n",
    "        pyplot.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datapath = os.path.join('..','..','data', 'Data_advanced.csv')\n",
    "\n",
    "    nn_network = RNN(datapath)\n",
    "    nn_network.build_model()\n",
    "    nn_network.train_network()\n",
    "    nn_network.predict()\n",
    "    nn_network.visualize()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
